{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results raport - Lending Club Data Default Prediction\n",
    "\n",
    "#### Antoni Szczepanik,  03.06.2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data contains complete loan data for all loans issued through years 2007-2015. We are presented with nearly 900 000 observations and 75 features. Their descriptions are present in Dictionary excel file. Our goal is to do a binary classificaion task and extract as much information out of every feature as possible. I decided to compare performance of three models. The simple overview of my approch is:\n",
    "\n",
    "\n",
    "**1. Extensive data preproscessing**\n",
    "\n",
    "    A. Removing irrelevant features\n",
    "    \n",
    "    B. Imputing missing data\n",
    "    \n",
    "    C. Train/Test Splitting\n",
    "    \n",
    "    D. Sampling\n",
    "    \n",
    "    \n",
    "**2. Model building**\n",
    "\n",
    "    A. Logistic Regression\n",
    "    \n",
    "    B. Random Forest\n",
    "    \n",
    "    C. XGBoost\n",
    "    \n",
    "    \n",
    "**3. Evaluating presented models**\n",
    "\n",
    "    A. Metric used\n",
    "    \n",
    "    B. Model evaluation\n",
    "    \n",
    "    \n",
    "    \n",
    "Final results can be found in summary, at the bottom of this notebook. Complete description of the process is provided below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing decription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First completely irrelevant features (according to feature dictionary provided) were removed. \n",
    "\n",
    "2. Features with more than 20% missing values would be hard and time consuming to make use of, so they were also removed.  \n",
    "\n",
    "3. Out of features which have less than 0.01% missing values rows which contained them were deleted.\n",
    "\n",
    "4. 5 features with missing values were still left. All missing values left were imputed using two techniques depending on the feature  - most common value and mean. \n",
    "\n",
    "5. Categorical features were converted to numerical. The type of conversion was specific to every feature, however few techniques dominated:\n",
    "\n",
    "\n",
    "    1. Dates were converted to time that passed since (in months or in years)\n",
    "    2. Categories to numbers (binary or integers)\n",
    "    3. Categories to dummy variables via OneHotEncoding\n",
    "6. Finnaly, dependence of  all features was **tested with chi2 test**. All features that scored p-values over 0.01 treshold were removed.  \n",
    "\n",
    "After that, we were left with a 61 columns. 25 of them were engeeniered and the remaining were originally in dataset. All of our features were numeric. We had 886 713 obserwations to train and test the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Testing split\n",
    "The dataset was splitted as stated in task description. However dataset is very imbalanced.\n",
    "That's why, even though random, the testing set contains **30% of positive class** and **30% of negative class** samples. This way the testing dataset has the same positive/negative ratio as our main dataset. The datasets were shuffled - good practice. \n",
    "\n",
    "### Sampling\n",
    "Training on imbalanced training set is not effective. That's why undersampled and oversampled datasets were created based on normal training dataset. It is important to notice that **original testing dataset is not used here** - it will be used only for testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Models\n",
    "The reason behind picking these particular models is that all 3 are among the most popular and best performing methods for classification tasks. Logistic regression is simple, however Random Forest and XGBoost are considered state of the art in supervised binary classification. \n",
    "\n",
    "### Logistic Regression\n",
    "Out of three models first one is **Logistic Regression**. It is simple, yet very effective and easily interpretable model. It is trained on imbalanced, undersampled and oversampled training datasets. The metric considered for model performance are ROC curves and area under these curves. The choice is explained in evaluation part. Logistic Regression is a good benchmark for the other models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/LogRegImbalanced.png)![title](images/LogRegOversampled.png)![title](images/LogRegUndersampled.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "The second model used was **Random Forest Classifier**. The results are as following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![title](images/RandForestUndersampled.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "The last model used in these analysis is **XGBoost**. It is known to achieve state of the art results in classification task. The result:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![title](images/XGBoostUndersampled.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "#### Metric\n",
    "The metric that is considered crucial in this task is Area Under ROC Curve. In case of default prediction with thousands of clients we want to maximize the True Positive Rate - we want to know when clients default is highly probable.  At the same time we want the False Positive Rate to be as small as possible - we don't want to reject clients. That is why model with the highest AUC ROC is picked. All the models are tested on unseen and imbalanced testing data. \n",
    "\n",
    "#### Model evaluation\n",
    "Out of three models logistic regression performed the worst. It performance was reasonable with AUC of 0.864, but it seems that we have enough information in our data build a far better model. \n",
    "\n",
    "Random forest scored nearly a perfect score with AUC of .992 It seems that our data contains high quality information, and that Random Forest is capable of extracting it nearly perfectly.  It means that randomly chosen positive class sample would have higher probability of beaing classified as possitive, rather than negative in 99,2% of our test observations. \n",
    "\n",
    "XGBoost algorithm seems to also tap on the information Random Forest captures and resulted an AUC of 0.987. The margin is very small, but on this given test set Random Forest performed better. That is why we don't have reason to belive it would score better otherwise. \n",
    "\n",
    "That is why **Random Forest** model is proposed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
